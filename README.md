# ğŸ¤– Central Critic Multi-Agent in Reinforcement Learning

A project that explores the power of centralized training and decentralized execution in multi-agent reinforcement learning using a Central Critic architecture. Implemented with MADDPG (Multi-Agent Deep Deterministic Policy Gradient), this system enables agents to collaborate or compete efficiently in complex environments.

---

## ğŸ“– Project Description

This project implements a **Central Critic architecture** within a **Multi-Agent Reinforcement Learning (MARL)** framework. Each agent independently learns its policy (actor), while a shared **centralized critic** evaluates their actions using the global state and all agent actions. This architecture helps stabilize learning in environments where agent policies are continuously changing (non-stationary).

Using the **MADDPG algorithm**, agents are trained in simulated environments like **Multi-Agent Particle Environment (MPE)** and **PyBullet**, allowing for experimentation with both cooperative and competitive tasks.

---

## âš™ï¸ Key Features

- âœ… Centralized training with decentralized execution
- ğŸ¯ Implements Multi-Agent DDPG (MADDPG)
- ğŸŒ Compatible with MPE and PyBullet environments
- ğŸ“ˆ Evaluation and visualization of agent behavior
- ğŸ§  Solves cooperative and adversarial scenarios

---

## ğŸ› ï¸ Technologies Used

- Python
- PyTorch / TensorFlow
- OpenAI Gym
- Multi-Agent Particle Environment (MPE)
- PyBullet
- NumPy, Matplotlib

---

